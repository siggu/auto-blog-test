"""
AI ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ ë° Notion ë°ì´í„°ë² ì´ìŠ¤ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ì›¹ì—ì„œ ìµœì‹  AI ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘ (RSS í”¼ë“œ ë˜ëŠ” ì›¹ ìŠ¤í¬ë˜í•‘)
2. Claude APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ë¶„ì„ ë° ë¶„ë¥˜
3. Notion ë°ì´í„°ë² ì´ìŠ¤ì— ìë™ ì—…ë¡œë“œ

ì‚¬ìš© ì „ ì„¤ì •:
1. .env íŒŒì¼ì— API í‚¤ ì„¤ì •
2. í¬ë¡ ì¡ ë˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ì •ê¸° ì‹¤í–‰ ì„¤ì •
"""

import os
import json
import requests
from datetime import datetime, timedelta
from typing import Optional
import feedparser
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# =============================================================================
# ì„¤ì •
# =============================================================================

NOTION_API_KEY = os.getenv("NOTION_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Notion ë°ì´í„°ë² ì´ìŠ¤ ID (ìƒì„±ëœ ë°ì´í„°ë² ì´ìŠ¤)
DATABASE_ID = "3e6b5982ea584534afa6618150f29d21"

# AI ë‰´ìŠ¤ RSS í”¼ë“œ ëª©ë¡
RSS_FEEDS = [
    {
        "name": "AIíƒ€ì„ìŠ¤",
        "url": "https://www.aitimes.com/rss/allArticle.xml",
        "language": "ko",
    },
    {
        "name": "ì¸ê³µì§€ëŠ¥ì‹ ë¬¸",
        "url": "https://www.aitimes.kr/rss/allArticle.xml",
        "language": "ko",
    },
    {
        "name": "MIT Tech Review AI",
        "url": "https://www.technologyreview.com/topic/artificial-intelligence/feed",
        "language": "en",
    },
    {
        "name": "VentureBeat AI",
        "url": "https://venturebeat.com/category/ai/feed/",
        "language": "en",
    },
    {
        "name": "The Verge AI",
        "url": "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml",
        "language": "en",
    },
]

# ê´€ë ¨ ê¸°ìˆ  í‚¤ì›Œë“œ ë§¤í•‘
TECH_KEYWORDS = {
    "LLM": [
        "llm",
        "large language model",
        "gpt",
        "claude",
        "gemini",
        "ëŒ€í˜•ì–¸ì–´ëª¨ë¸",
        "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸",
        "chatgpt",
    ],
    "ì´ë¯¸ì§€ ìƒì„±": [
        "image generation",
        "dall-e",
        "midjourney",
        "stable diffusion",
        "ì´ë¯¸ì§€ ìƒì„±",
        "ê·¸ë¦¼ ìƒì„±",
        "text-to-image",
    ],
    "ì¶”ë¡  AI": [
        "reasoning",
        "o1",
        "o3",
        "chain of thought",
        "ì¶”ë¡ ",
        "ì‚¬ê³ ",
        "thinking",
    ],
    "ì—ì´ì „íŠ¸": ["agent", "agentic", "ì—ì´ì „íŠ¸", "ììœ¨ ì—ì´ì „íŠ¸", "autonomous"],
    "ë©€í‹°ëª¨ë‹¬": ["multimodal", "vision", "audio", "ë©€í‹°ëª¨ë‹¬", "ë‹¤ì¤‘ëª¨ë‹¬", "ë¹„ì „"],
    "ì˜¤í”ˆì†ŒìŠ¤": ["open source", "ì˜¤í”ˆì†ŒìŠ¤", "opensource", "hugging face", "í—ˆê¹…í˜ì´ìŠ¤"],
    "ê°•í™”í•™ìŠµ": ["reinforcement learning", "rl", "rlhf", "ê°•í™”í•™ìŠµ", "ë³´ìƒ ëª¨ë¸"],
    "ë¡œë³´í‹±ìŠ¤": ["robot", "robotics", "ë¡œë´‡", "ë¡œë³´í‹±ìŠ¤", "embodied ai"],
    "ìŒì„±/ì˜¤ë””ì˜¤": [
        "voice",
        "audio",
        "speech",
        "tts",
        "stt",
        "ìŒì„±",
        "ì˜¤ë””ì˜¤",
        "whisper",
    ],
}

# ê¸°ì—…/ê¸°ê´€ í‚¤ì›Œë“œ ë§¤í•‘
ORG_KEYWORDS = {
    "OpenAI": [
        "openai",
        "chatgpt",
        "gpt-4",
        "gpt-5",
        "dall-e",
        "sam altman",
        "ìƒ˜ ì˜¬íŠ¸ë¨¼",
    ],
    "Google": ["google", "êµ¬ê¸€", "deepmind", "ë”¥ë§ˆì¸ë“œ", "gemini", "ì œë¯¸ë‚˜ì´", "bard"],
    "Anthropic": ["anthropic", "ì•¤ìŠ¤ë¡œí”½", "claude", "í´ë¡œë“œ"],
    "Meta": ["meta", "ë©”íƒ€", "facebook", "llama", "ë¼ë§ˆ"],
    "Microsoft": ["microsoft", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "copilot", "ì½”íŒŒì¼ëŸ¿", "azure"],
    "NVIDIA": ["nvidia", "ì—”ë¹„ë””ì•„", "cuda", "gpu", "h100", "blackwell"],
    "êµ­ë‚´ ì—°êµ¬ê¸°ê´€": [
        "kaist",
        "ì¹´ì´ìŠ¤íŠ¸",
        "ì„œìš¸ëŒ€",
        "postech",
        "í¬ìŠ¤í…",
        "unist",
        "etri",
        "í•œêµ­ì „ìí†µì‹ ì—°êµ¬ì›",
    ],
}


# =============================================================================
# Notion API í´ë¼ì´ì–¸íŠ¸
# =============================================================================


class NotionClient:
    """Notion API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.notion.com/v1"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28",
        }

    def create_page(
        self, database_id: str, properties: dict, news_data: dict = None
    ) -> dict:
        """ë°ì´í„°ë² ì´ìŠ¤ì— ìƒˆ í˜ì´ì§€ ìƒì„± (ì›ë¬¸ ë³´ì¡´)"""
        url = f"{self.base_url}/pages"

        data = {"parent": {"database_id": database_id}, "properties": properties}

        # ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í˜ì´ì§€ ë‚´ìš© êµ¬ì„±
        if news_data:
            children = []

            # 1. ìš”ì•½ ì„¹ì…˜ (AI ë¶„ì„ ê²°ê³¼)
            summary = news_data.get("summary", "ìš”ì•½ ì—†ìŒ")
            if summary:
                children.append(
                    {
                        "object": "block",
                        "type": "callout",
                        "callout": {
                            "rich_text": [
                                {"type": "text", "text": {"content": summary}}
                            ],
                            "icon": {"emoji": "ğŸ’¡"},
                            "color": "blue_background",
                        },
                    }
                )

            # êµ¬ë¶„ì„ 
            children.append({"object": "block", "type": "divider", "divider": {}})

            # 2. ì´ë¯¸ì§€ í‘œì‹œ (ìµœëŒ€ 3ê°œ)
            all_images = news_data.get("all_images", [])
            if not all_images and news_data.get("image_url"):
                all_images = [news_data.get("image_url")]

            for img_url in all_images[:3]:  # ìµœëŒ€ 3ê°œ
                if img_url:
                    children.append(
                        {
                            "object": "block",
                            "type": "image",
                            "image": {"type": "external", "external": {"url": img_url}},
                        }
                    )

            # 3. í•µì‹¬ ë‚´ìš© (ì›ë¬¸ì—ì„œ ì¶”ì¶œí•œ ë¬¸ì¥ë“¤)
            key_sentences = news_data.get("key_sentences", [])
            if key_sentences:
                children.append(
                    {
                        "object": "block",
                        "type": "heading_3",
                        "heading_3": {
                            "rich_text": [
                                {"type": "text", "text": {"content": "í•µì‹¬ ë‚´ìš©"}}
                            ]
                        },
                    }
                )

                # ê° í•µì‹¬ ë¬¸ì¥ì„ ì¸ìš© ë¸”ë¡ìœ¼ë¡œ í‘œì‹œ
                for sentence in key_sentences[:5]:  # ìµœëŒ€ 5ë¬¸ì¥
                    if sentence and sentence.strip():
                        children.append(
                            {
                                "object": "block",
                                "type": "quote",
                                "quote": {
                                    "rich_text": [
                                        {
                                            "type": "text",
                                            "text": {
                                                "content": sentence.strip()[:2000]
                                            },
                                        }
                                    ],
                                    "color": "default",
                                },
                            }
                        )

            # êµ¬ë¶„ì„ 
            children.append({"object": "block", "type": "divider", "divider": {}})

            # 4. ì›ë¬¸ ë§í¬
            if news_data.get("link"):
                children.append(
                    {
                        "object": "block",
                        "type": "bookmark",
                        "bookmark": {"url": news_data.get("link", "")},
                    }
                )

            # 5. ë©”íƒ€ ì •ë³´
            children.append(
                {
                    "object": "block",
                    "type": "callout",
                    "callout": {
                        "rich_text": [
                            {
                                "type": "text",
                                "text": {
                                    "content": f"ë°œí–‰ì¼: {news_data.get('date', 'N/A')}  |  ì¶œì²˜: {news_data.get('source', 'N/A')}"
                                },
                            }
                        ],
                        "icon": {"emoji": "ğŸ“„"},
                        "color": "gray_background",
                    },
                }
            )

            data["children"] = children

        response = requests.post(url, headers=self.headers, json=data)
        return response.json()

    def _split_into_paragraphs(self, text: str, sentences_per_para: int = 3) -> list:
        """ì›ë¬¸ì„ ë‹¨ë½ìœ¼ë¡œ ë¶„ë¦¬ (ë‚´ìš© ìˆ˜ì • ì—†ì´ ê°€ë…ì„±ë§Œ í–¥ìƒ)"""
        import re

        if not text:
            return []

        # ì´ë¯¸ ë‹¨ë½ êµ¬ë¶„ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if "\n\n" in text:
            paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
            # ë„ˆë¬´ ì§§ì€ ë‹¨ë½ì€ í•©ì¹˜ê¸°
            merged = []
            current = ""
            for p in paragraphs:
                if len(p) < 50 and current:
                    current += " " + p
                else:
                    if current:
                        merged.append(current)
                    current = p
            if current:
                merged.append(current)
            return merged

        if "\n" in text:
            lines = [p.strip() for p in text.split("\n") if p.strip()]
            # í•œ ì¤„ì”© ìˆìœ¼ë©´ 2-3ì¤„ì”© í•©ì¹˜ê¸°
            merged = []
            current = ""
            for line in lines:
                if len(current) + len(line) < 300:
                    current = (current + " " + line).strip() if current else line
                else:
                    if current:
                        merged.append(current)
                    current = line
            if current:
                merged.append(current)
            return merged

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ ë¶€í˜¸ ê³ ë ¤)
        sentences = re.split(r"(?<=[.!?ã€‚])\s+", text)

        # Nê°œ ë¬¸ì¥ì”© ë¬¶ì–´ì„œ ë‹¨ë½ ìƒì„±
        paragraphs = []
        current_para = []

        for sentence in sentences:
            current_para.append(sentence)
            if len(current_para) >= sentences_per_para:
                paragraphs.append(" ".join(current_para))
                current_para = []

        # ë‚¨ì€ ë¬¸ì¥ ì²˜ë¦¬
        if current_para:
            paragraphs.append(" ".join(current_para))

        return paragraphs

    def query_database(self, database_id: str, filter_obj: dict = None) -> dict:
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬"""
        url = f"{self.base_url}/databases/{database_id}/query"
        data = {}
        if filter_obj:
            data["filter"] = filter_obj

        response = requests.post(url, headers=self.headers, json=data)
        return response.json()

    def check_duplicate(self, database_id: str, title: str) -> bool:
        """ì¤‘ë³µ ê¸°ì‚¬ ì²´í¬"""
        filter_obj = {
            "property": "ì œëª©",
            "title": {"contains": title[:50]},  # ì œëª© ì¼ë¶€ë¡œ ê²€ìƒ‰
        }
        result = self.query_database(database_id, filter_obj)
        return len(result.get("results", [])) > 0


# =============================================================================
# ë‰´ìŠ¤ ë¶„ì„ê¸° (OpenAI / Claude API ì„ íƒ ê°€ëŠ¥)
# =============================================================================


class NewsAnalyzer:
    """AI APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ë¶„ì„ (OpenAI ë˜ëŠ” Claude)"""

    def __init__(self, api_key: str, provider: str = "openai"):
        """
        Args:
            api_key: API í‚¤
            provider: "openai" (ê¸°ë³¸) ë˜ëŠ” "claude"
        """
        self.api_key = api_key
        self.provider = provider.lower()

        if self.provider == "claude":
            self.base_url = "https://api.anthropic.com/v1/messages"
            self.model = "claude-sonnet-4-20250514"
        else:
            self.base_url = "https://api.openai.com/v1/chat/completions"
            self.model = "gpt-5-nano"  # ê°€ì¥ ì €ë ´í•œ ëª¨ë¸ ($0.05/$0.40 per 1M tokens)

    def analyze_news(self, title: str, content: str) -> dict:
        """ë‰´ìŠ¤ ë¶„ì„ ë° ë¶„ë¥˜ (ì›ë¬¸ ë³´ì¡´)"""

        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ê°€ AI/ì¸ê³µì§€ëŠ¥ **ê¸°ìˆ ** ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {content[:4000]}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "is_ai_related": true ë˜ëŠ” false,
    "rejection_reason": "AI ê´€ë ¨ ì—†ëŠ” ê²½ìš° ì´ìœ ",
    "summary": "2-3ë¬¸ì¥ ìš”ì•½ (í•œêµ­ì–´)",
    "key_sentences": ["ì›ë¬¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ 1", "ì›ë¬¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ 2", ...],
    "technologies": ["LLM", "ì´ë¯¸ì§€ ìƒì„±", "ì¶”ë¡  AI", "ì—ì´ì „íŠ¸", "ë©€í‹°ëª¨ë‹¬", "ì˜¤í”ˆì†ŒìŠ¤", "ê°•í™”í•™ìŠµ", "ë¡œë³´í‹±ìŠ¤", "ìŒì„±/ì˜¤ë””ì˜¤" ì¤‘ ì„ íƒ],
    "organization": "OpenAI, Google, Anthropic, Meta, Microsoft, NVIDIA, êµ­ë‚´ ì—°êµ¬ê¸°ê´€, ê¸°íƒ€ ì¤‘ ì„ íƒ",
    "importance": "ğŸ”¥ ì£¼ìš”, ğŸ“Œ ì¼ë°˜, ğŸ“ ì°¸ê³  ì¤‘ ì„ íƒ"
}}

**key_sentences ê·œì¹™ (ë§¤ìš° ì¤‘ìš”):**
- ì›ë¬¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì¥ì„ **ê·¸ëŒ€ë¡œ ë³µì‚¬**
- ìµœì†Œ 1ë¬¸ì¥, ìµœëŒ€ 5ë¬¸ì¥
- ì ˆëŒ€ ìˆ˜ì •í•˜ê±°ë‚˜ ìš”ì•½í•˜ì§€ ë§ê³ , ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
- ê¸°ì‚¬ì˜ í•µì‹¬ ì •ë³´ë¥¼ ë‹´ì€ ë¬¸ì¥ ì„ íƒ

**key_sentences ì œì™¸ ëŒ€ìƒ:**
- ì´ë¯¸ì§€ ìº¡ì…˜/ì„¤ëª… (ì˜ˆ: "ì‚¬ì§„=...", "(ì‚¬ì§„:...)", "ì´ë¯¸ì§€:...", "ì¶œì²˜=...")
- ê¸°ì ì •ë³´, ì €ì‘ê¶Œ ë¬¸êµ¬
- ë‚ ì§œ/ì¥ì†Œë§Œ ìˆëŠ” ë¬¸ì¥

**AI ê´€ë ¨ì„± íŒë‹¨:**
âœ… AI ê´€ë ¨: AI ê¸°ìˆ /ì—°êµ¬, AI ê¸°ì—… ë™í–¥, AI ì •ì±…/ê·œì œ, AI ì œí’ˆ/ì„œë¹„ìŠ¤
âŒ AI ë¹„ê´€ë ¨: AIì›¹íˆ°/ë§Œí™” (AI ìƒì„± ì½˜í…ì¸ ), ì—°ì˜ˆ/ìŠ¤í¬ì¸ 

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            if self.provider == "openai":
                response = self._call_openai(prompt)
            else:
                response = self._call_claude(prompt)

            if response:
                # key_sentencesì—ì„œ ì´ë¯¸ì§€ ìº¡ì…˜ í•„í„°ë§
                if "key_sentences" in response:
                    response["key_sentences"] = self._filter_image_captions(
                        response["key_sentences"]
                    )
                return response
        except Exception as e:
            print(f"ë¶„ì„ ì˜¤ë¥˜: {e}")

        # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        return self._fallback_analysis(title, content)

    def _filter_image_captions(self, sentences: list) -> list:
        """ì´ë¯¸ì§€ ìº¡ì…˜/ì„¤ëª… ë¬¸ì¥ í•„í„°ë§"""
        import re

        if not sentences:
            return []

        # ì´ë¯¸ì§€ ìº¡ì…˜ íŒ¨í„´
        caption_patterns = [
            r"^ì‚¬ì§„[=:]",
            r"^\(ì‚¬ì§„[=:]",
            r"^ì´ë¯¸ì§€[=:]",
            r"^\(ì´ë¯¸ì§€[=:]",
            r"^ì¶œì²˜[=:]",
            r"^\(ì¶œì²˜[=:]",
            r"^ì‚¬ì§„ ì œê³µ",
            r"ë³¸ì§€\s*DB",
            r"ì œê³µ\s*ì‚¬ì§„",
            r"ìº¡ì²˜\s*í™”ë©´",
            r"ìŠ¤í¬ë¦°ìƒ·",
            r"^â–²",
            r"^\[ì‚¬ì§„\]",
            r"AI\s*ìƒì„±.*ì´ë¯¸ì§€",
            r"ì´ë¯¸ì§€.*AI\s*ìƒì„±",
        ]

        filtered = []
        for sentence in sentences:
            if not sentence or not sentence.strip():
                continue

            sentence = sentence.strip()

            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì´ë¯¸ì§€ ìº¡ì…˜ ì œì™¸
            is_caption = False
            for pattern in caption_patterns:
                if re.search(pattern, sentence, re.IGNORECASE):
                    is_caption = True
                    break

            # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸ (20ì ë¯¸ë§Œ)
            if len(sentence) < 20:
                is_caption = True

            if not is_caption:
                filtered.append(sentence)

        return filtered[:5]  # ìµœëŒ€ 5ë¬¸ì¥

    def _call_openai(self, prompt: str) -> dict:
        """OpenAI API í˜¸ì¶œ"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "max_completion_tokens": 1000,  # GPT-5 ëª¨ë¸ì€ max_completion_tokens ì‚¬ìš©, temperature ë¯¸ì§€ì›
        }

        response = requests.post(self.base_url, headers=headers, json=data)

        if response.status_code != 200:
            print(f"OpenAI API ì˜¤ë¥˜ ({response.status_code}): {response.text[:200]}")
            return None

        result = response.json()

        if "choices" in result and len(result["choices"]) > 0:
            text = result["choices"][0]["message"]["content"]
            return self._parse_json_response(text)

        return None

    def _call_claude(self, prompt: str) -> dict:
        """Claude API í˜¸ì¶œ"""
        headers = {
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json",
        }

        data = {
            "model": self.model,
            "max_tokens": 1000,
            "messages": [{"role": "user", "content": prompt}],
        }

        response = requests.post(self.base_url, headers=headers, json=data)

        if response.status_code != 200:
            print(f"Claude API ì˜¤ë¥˜ ({response.status_code}): {response.text[:200]}")
            return None

        result = response.json()

        if "content" in result and len(result["content"]) > 0:
            text = result["content"][0]["text"]
            return self._parse_json_response(text)

        return None

    def _parse_json_response(self, text: str) -> dict:
        """JSON ì‘ë‹µ íŒŒì‹±"""
        import re

        if not text:
            return None

        # ë””ë²„ê¹…: ì‘ë‹µ ì•ë¶€ë¶„ ì¶œë ¥
        # print(f"DEBUG ì‘ë‹µ: {text[:500]}")

        # 1. ì§ì ‘ íŒŒì‹± ì‹œë„
        try:
            return json.loads(text)
        except:
            pass

        # 2. JSON ë¸”ë¡ ì¶”ì¶œ (```json ... ``` í˜•ì‹)
        json_match = re.search(r"```json\s*([\s\S]*?)\s*```", text)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except:
                pass

        # 3. ``` ... ``` í˜•ì‹ (json í‘œì‹œ ì—†ì´)
        json_match = re.search(r"```\s*([\s\S]*?)\s*```", text)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except:
                pass

        # 4. ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” JSON ì°¾ê¸° (ê°€ì¥ ë°”ê¹¥ìª½ ì¤‘ê´„í˜¸)
        json_match = re.search(r"\{[\s\S]*\}", text)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except:
                # JSON ë‚´ë¶€ì˜ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ì‹œë„
                json_str = json_match.group(0)
                # ì¤„ë°”ê¿ˆì„ ì´ìŠ¤ì¼€ì´í”„
                json_str = json_str.replace("\n", "\\n")
                try:
                    return json.loads(json_str)
                except:
                    pass

        # 5. í‚¤-ê°’ íŒ¨í„´ìœ¼ë¡œ ìˆ˜ë™ ì¶”ì¶œ ì‹œë„
        try:
            result = {}

            # is_ai_related ì¶”ì¶œ
            ai_match = re.search(
                r'"is_ai_related"\s*:\s*(true|false)', text, re.IGNORECASE
            )
            if ai_match:
                result["is_ai_related"] = ai_match.group(1).lower() == "true"

            # rejection_reason ì¶”ì¶œ
            reason_match = re.search(r'"rejection_reason"\s*:\s*"([^"]*)"', text)
            if reason_match:
                result["rejection_reason"] = reason_match.group(1)

            # summary ì¶”ì¶œ
            summary_match = re.search(r'"summary"\s*:\s*"([^"]*)"', text)
            if summary_match:
                result["summary"] = summary_match.group(1)

            # importance ì¶”ì¶œ
            importance_match = re.search(r'"importance"\s*:\s*"([^"]*)"', text)
            if importance_match:
                result["importance"] = importance_match.group(1)

            # organization ì¶”ì¶œ
            org_match = re.search(r'"organization"\s*:\s*"([^"]*)"', text)
            if org_match:
                result["organization"] = org_match.group(1)

            if "is_ai_related" in result:
                # ê¸°ë³¸ê°’ ì„¤ì •
                result.setdefault("rejection_reason", "")
                result.setdefault("summary", "")
                result.setdefault("key_sentences", [])
                result.setdefault("technologies", ["ê¸°íƒ€"])
                result.setdefault("organization", "ê¸°íƒ€")
                result.setdefault("importance", "ğŸ“Œ ì¼ë°˜")
                return result
        except:
            pass

        print(f"JSON ì¶”ì¶œ ì‹¤íŒ¨. ì‘ë‹µ: {text[:100]}...")
        return None

    def _fallback_analysis(self, title: str, content: str) -> dict:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ë¶„ì„"""
        text = (title + " " + content).lower()

        # AI ê´€ë ¨ì„± ì²´í¬ (í‚¤ì›Œë“œ ê¸°ë°˜)
        ai_keywords = [
            "ai",
            "artificial intelligence",
            "ì¸ê³µì§€ëŠ¥",
            "machine learning",
            "ë¨¸ì‹ ëŸ¬ë‹",
            "deep learning",
            "ë”¥ëŸ¬ë‹",
            "neural network",
            "ì‹ ê²½ë§",
            "llm",
            "gpt",
            "claude",
            "gemini",
            "chatgpt",
            "openai",
            "anthropic",
            "transformer",
            "ìì—°ì–´ì²˜ë¦¬",
            "nlp",
            "computer vision",
            "ì»´í“¨í„° ë¹„ì „",
            "reinforcement learning",
            "ê°•í™”í•™ìŠµ",
            "generative ai",
            "ìƒì„±í˜• ai",
            "foundation model",
            "íŒŒìš´ë°ì´ì…˜ ëª¨ë¸",
            "nvidia",
            "ì—”ë¹„ë””ì•„",
            "gpu",
            "cuda",
            "tensor",
            "í…ì„œ",
            "ì¶”ë¡ ",
            "inference",
        ]

        # ë¹„AI í‚¤ì›Œë“œ (ì œì™¸ ëŒ€ìƒ)
        non_ai_keywords = [
            "ê²°í˜¼",
            "ì´í˜¼",
            "ì—´ì• ",
            "ì—°ì˜ˆ",
            "ì•„ì´ëŒ",
            "ë“œë¼ë§ˆ",
            "ì˜ˆëŠ¥",
            "ê°€ìˆ˜",
            "ë°°ìš°",
            "ì¶•êµ¬",
            "ì•¼êµ¬",
            "ë†êµ¬",
            "ì˜¬ë¦¼í”½",
            "ì›”ë“œì»µ",
            "ê²½ê¸° ê²°ê³¼",
            "ìŠ¹ë¦¬",
            "íŒ¨ë°°",
            "ë‚ ì”¨",
            "ê¸°ì˜¨",
            "ê°•ìˆ˜ëŸ‰",
            "ë¯¸ì„¸ë¨¼ì§€",
        ]

        # ì œëª© ê¸°ë°˜ í•„í„°ë§ (AIë¡œ ë§Œë“  ì½˜í…ì¸ ëŠ” AI ê¸°ìˆ  ë‰´ìŠ¤ê°€ ì•„ë‹˜)
        title_lower = title.lower()
        ai_content_patterns = [
            "aiì›¹íˆ°",
            "aië§Œí™”",
            "ai ì›¹íˆ°",
            "ai ë§Œí™”",
            "aiê·¸ë¦¼",
            "ai ê·¸ë¦¼",
            "aiì´ë¯¸ì§€",
            "ai ì´ë¯¸ì§€",
            "aiì˜ìƒ",
            "ai ì˜ìƒ",
            "aiì´ìŠˆíŠ¸ë Œë“œ",
            "ai ì´ìŠˆíŠ¸ë Œë“œ",
            "ai ì´ìŠˆ íŠ¸ë Œë“œ",
            "[aiì›¹íˆ°]",
            "[aië§Œí™”]",
            "[ai ì›¹íˆ°]",
            "[ai ë§Œí™”]",
        ]

        is_ai_generated_content = any(
            pattern in title_lower for pattern in ai_content_patterns
        )

        has_ai_keyword = any(keyword in text for keyword in ai_keywords)
        has_non_ai_keyword = any(keyword in text for keyword in non_ai_keywords)

        # AI í‚¤ì›Œë“œê°€ ìˆê³  ë¹„AI í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê´€ë ¨
        # ë‹¨, AIë¡œ ë§Œë“  ì½˜í…ì¸ (ì›¹íˆ°, ë§Œí™” ë“±)ëŠ” ì œì™¸
        is_ai_related = (
            has_ai_keyword and not has_non_ai_keyword and not is_ai_generated_content
        )

        # ê±°ë¶€ ì‚¬ìœ  ì„¤ì •
        if is_ai_generated_content:
            rejection_reason = "AIë¡œ ë§Œë“  ì½˜í…ì¸  (AI ê¸°ìˆ  ë‰´ìŠ¤ ì•„ë‹˜)"
        elif has_non_ai_keyword:
            rejection_reason = "ë¹„AI ê´€ë ¨ ì½˜í…ì¸  (ì—°ì˜ˆ/ìŠ¤í¬ì¸ /ì¼ë°˜)"
        elif not has_ai_keyword:
            rejection_reason = "AI ê´€ë ¨ í‚¤ì›Œë“œ ì—†ìŒ"
        else:
            rejection_reason = ""

        # ê¸°ìˆ  ë¶„ë¥˜
        technologies = []
        for tech, keywords in TECH_KEYWORDS.items():
            if any(kw in text for kw in keywords):
                technologies.append(tech)

        # ê¸°ê´€ ë¶„ë¥˜
        organization = "ê¸°íƒ€"
        for org, keywords in ORG_KEYWORDS.items():
            if any(kw in text for kw in keywords):
                organization = org
                break

        # í´ë°±ìš© í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ (ì´ë¯¸ì§€ ìº¡ì…˜ ì œì™¸)
        import re

        sentences = re.split(r"[.!?ã€‚]\s+", content)
        raw_sentences = [
            s.strip() + "." for s in sentences if s.strip() and len(s.strip()) > 20
        ]
        key_sentences = self._filter_image_captions(raw_sentences)[:2]

        return {
            "is_ai_related": is_ai_related,
            "rejection_reason": rejection_reason,
            "summary": title,
            "key_sentences": key_sentences,
            "technologies": technologies[:3] if technologies else ["ê¸°íƒ€"],
            "organization": organization,
            "importance": "ğŸ“Œ ì¼ë°˜",
        }


# =============================================================================
# ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
# =============================================================================


class NewsCollector:
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""

    def __init__(self, feeds: list):
        self.feeds = feeds

    def collect_news(self, days: int = 1) -> list:
        """ìµœê·¼ Nì¼ ì´ë‚´ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        from datetime import timezone

        cutoff_date = datetime.now(timezone.utc).replace(
            tzinfo=None
        )  # UTC ê¸°ì¤€, naiveë¡œ ë³€í™˜
        cutoff_date = cutoff_date - timedelta(days=days)
        all_news = []

        for feed_info in self.feeds:
            try:
                # User-Agent í—¤ë” ì¶”ê°€ (ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œ í•„ìš”)
                feed = feedparser.parse(
                    feed_info["url"],
                    agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                )

                for entry in feed.entries[:10]:  # ê° í”¼ë“œì—ì„œ ìµœëŒ€ 10ê°œ
                    # ë‚ ì§œ íŒŒì‹± (ë‹¤ì–‘í•œ í•„ë“œ ì‹œë„)
                    pub_date = self._parse_date(entry)

                    # íƒ€ì„ì¡´ ì •ë³´ ì œê±° (naive datetimeìœ¼ë¡œ í†µì¼)
                    if pub_date.tzinfo is not None:
                        pub_date = pub_date.replace(tzinfo=None)

                    # ê¸°ê°„ í•„í„°
                    if pub_date < cutoff_date:
                        continue

                    # ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì¶”ì¶œ
                    content_data = self._get_content(entry)

                    news_item = {
                        "title": entry.get("title", ""),
                        "link": entry.get("link", ""),
                        "content": content_data.get("content", ""),
                        "image_url": content_data.get("image_url"),
                        "all_images": content_data.get("all_images", []),  # ëª¨ë“  ì´ë¯¸ì§€
                        "date": pub_date.strftime("%Y-%m-%d"),
                        "source": feed_info["name"],
                    }
                    all_news.append(news_item)

                    # ë””ë²„ê·¸ ì¶œë ¥
                    img_count = len(content_data.get("all_images", []))
                    img_status = f"ğŸ–¼ï¸({img_count})" if img_count > 0 else "ğŸ“„"
                    print(
                        f"{img_status} {news_item['title'][:50]}... -> {news_item['date']}"
                    )

            except Exception as e:
                print(f"í”¼ë“œ ìˆ˜ì§‘ ì˜¤ë¥˜ ({feed_info['name']}): {e}")

        return all_news

    def _parse_date(self, entry) -> datetime:
        """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±"""
        from email.utils import parsedate_to_datetime
        import re

        # 1. published ë¬¸ìì—´ ë¨¼ì € ì‹œë„ (í•œêµ­ RSS í”¼ë“œëŠ” ëŒ€ë¶€ë¶„ ì´ í˜•ì‹)
        if hasattr(entry, "published") and entry.published:
            parsed = self._parse_date_string(entry.published)
            if parsed:
                return parsed

        # 2. published_parsed ì‹œë„
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            try:
                return datetime(*entry.published_parsed[:6])
            except:
                pass

        # 3. updated ë¬¸ìì—´ íŒŒì‹± ì‹œë„
        if hasattr(entry, "updated") and entry.updated:
            parsed = self._parse_date_string(entry.updated)
            if parsed:
                return parsed

        # 4. updated_parsed ì‹œë„
        if hasattr(entry, "updated_parsed") and entry.updated_parsed:
            try:
                return datetime(*entry.updated_parsed[:6])
            except:
                pass

        # 5. dc:date ì‹œë„ (Dublin Core)
        if hasattr(entry, "dc_date") and entry.dc_date:
            parsed = self._parse_date_string(entry.dc_date)
            if parsed:
                return parsed

        # í´ë°±: í˜„ì¬ ì‹œê°„
        return datetime.now()

    def _parse_date_string(self, date_str: str) -> Optional[datetime]:
        """ë¬¸ìì—´ ë‚ ì§œ íŒŒì‹±"""
        from email.utils import parsedate_to_datetime
        import re

        if not date_str:
            return None

        date_str = date_str.strip()

        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì‹œë„ (í•œêµ­ í˜•ì‹ ìš°ì„ )
        date_formats = [
            "%Y-%m-%d %H:%M:%S",  # í•œêµ­ RSS í˜•ì‹: 2025-12-25 19:09:25
            "%Y-%m-%d %H:%M",  # 2025-12-25 19:09
            "%Y-%m-%d",  # 2025-12-25
            "%Y.%m.%d %H:%M:%S",  # í•œêµ­ í˜•ì‹: 2025.12.25 19:09:25
            "%Y.%m.%d %H:%M",  # 2025.12.25 19:09
            "%Y.%m.%d",  # 2025.12.25
            "%Y/%m/%d %H:%M:%S",  # 2025/12/25 19:09:25
            "%Y/%m/%d",  # 2025/12/25
            "%Y-%m-%dT%H:%M:%S%z",  # ISO 8601
            "%Y-%m-%dT%H:%M:%SZ",  # ISO 8601 UTC
            "%Y-%m-%dT%H:%M:%S",  # ISO 8601 no tz
            "%d %b %Y %H:%M:%S",  # 25 Dec 2025 19:09:25
            "%a, %d %b %Y %H:%M:%S",  # Wed, 25 Dec 2025 19:09:25
        ]

        for fmt in date_formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue

        # +0900 ê°™ì€ íƒ€ì„ì¡´ ì œê±° í›„ ì¬ì‹œë„
        clean_date = re.sub(r"[+-]\d{4}$", "", date_str)
        clean_date = re.sub(r"\s+\w{3,4}$", "", clean_date)  # KST, GMT ë“± ì œê±°
        clean_date = clean_date.strip()

        for fmt in date_formats:
            try:
                return datetime.strptime(clean_date, fmt)
            except ValueError:
                continue

        # RFC 2822 í˜•ì‹ ì‹œë„ (ì˜ˆ: "Wed, 25 Dec 2024 10:30:00 +0900")
        try:
            return parsedate_to_datetime(date_str)
        except:
            pass

        return None

    def _get_content(self, entry) -> dict:
        """ê¸°ì‚¬ ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì¶”ì¶œ - RSS ë‚´ìš© + ì›¹ ìŠ¤í¬ë˜í•‘"""
        result = {"content": "", "image_url": None, "all_images": []}

        # ë¨¼ì € RSSì—ì„œ ê¸°ë³¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        rss_content = ""

        if hasattr(entry, "content") and entry.content:
            if isinstance(entry.content, list) and len(entry.content) > 0:
                rss_content = entry.content[0].get("value", "")

        if not rss_content and hasattr(entry, "summary") and entry.summary:
            rss_content = entry.summary

        if not rss_content and hasattr(entry, "description") and entry.description:
            rss_content = entry.description

        # ê¸°ì‚¬ ë§í¬ì—ì„œ ì „ì²´ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì‹œë„
        link = entry.get("link", "")
        if link:
            scraped = self._scrape_article(link)
            if scraped.get("content") and len(scraped["content"]) > len(
                self._strip_html(rss_content)
            ):
                result["content"] = scraped["content"]
            else:
                result["content"] = self._strip_html(rss_content)

            # ì´ë¯¸ì§€ URL ì €ì¥
            if scraped.get("image_url"):
                result["image_url"] = scraped["image_url"]
            if scraped.get("all_images"):
                result["all_images"] = scraped["all_images"]
        else:
            result["content"] = self._strip_html(rss_content)

        return result

    def _strip_html(self, html_content: str) -> str:
        """HTML íƒœê·¸ ì œê±°í•˜ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë°˜í™˜"""
        if not html_content:
            return ""

        try:
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(html_content, "html.parser")

            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup.select("script, style, nav, footer, aside, figure, img"):
                tag.decompose()

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = soup.get_text(separator="\n", strip=True)

            # ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
            import re

            text = re.sub(r"\n{3,}", "\n\n", text)
            text = re.sub(r" {2,}", " ", text)

            return text.strip()
        except ImportError:
            # BeautifulSoup ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì •ê·œì‹ìœ¼ë¡œ ì²˜ë¦¬
            import re

            text = re.sub(r"<[^>]+>", "", html_content)
            text = re.sub(r"\s+", " ", text)
            return text.strip()
        except:
            return html_content

    def _scrape_article(self, url: str) -> dict:
        """ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ê³¼ ëª¨ë“  ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘"""
        result = {"content": "", "image_url": None, "all_images": []}

        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # HTML íŒŒì‹±
            from bs4 import BeautifulSoup
            from urllib.parse import urljoin

            soup = BeautifulSoup(response.text, "html.parser")

            # ëª¨ë“  ì´ë¯¸ì§€ ì¶”ì¶œ
            all_images = self._extract_all_images(soup, url)
            if all_images:
                result["image_url"] = all_images[0]  # ì²« ë²ˆì§¸ëŠ” ëŒ€í‘œ ì´ë¯¸ì§€
                result["all_images"] = all_images  # ëª¨ë“  ì´ë¯¸ì§€

            # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒìë“¤ ì‹œë„
            content = None

            # AIíƒ€ì„ìŠ¤, ì¸ê³µì§€ëŠ¥ì‹ ë¬¸ ë“± í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            selectors = [
                "article#article-view-content-div",  # AIíƒ€ì„ìŠ¤
                "div#article-view-content-div",  # AIíƒ€ì„ìŠ¤
                "div.article-body",  # ì¼ë°˜
                "div.article_body",  # ì¼ë°˜
                "div.article-content",  # ì¼ë°˜
                "div.news-content",  # ì¼ë°˜
                "div.view_cont",  # ì¼ë¶€ í•œêµ­ ì‚¬ì´íŠ¸
                "div#articleBody",  # ì¼ë¶€ ì‚¬ì´íŠ¸
                "div.entry-content",  # WordPress
                "article.post-content",  # ë¸”ë¡œê·¸
                'div[itemprop="articleBody"]',  # Schema.org
                "article",  # ì¼ë°˜ article íƒœê·¸
            ]

            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for tag in element.select(
                        "script, style, nav, footer, aside, .ad, .advertisement, .social-share, .related-article, .related_article, .sns_share, .article-sns, .byline, .reporter-info, .copyright, .article-footer, .tag-group, .keyword, .article-tag"
                    ):
                        tag.decompose()

                    content = element.get_text(separator="\n", strip=True)
                    if content and len(content) > 200:
                        break

            if content:
                # ì½˜í…ì¸  ì •ë¦¬
                content = self._clean_article_content(content)
                result["content"] = content[:8000]  # ë” ë§ì€ ë‚´ìš© í¬í•¨

        except ImportError:
            print("âš ï¸ BeautifulSoup ë¯¸ì„¤ì¹˜. pip install beautifulsoup4 ì‹¤í–‰ í•„ìš”")
        except Exception as e:
            # ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ë„˜ì–´ê°
            pass

        return result

    def _clean_article_content(self, content: str) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„° ì œê±°"""
        import re

        if not content:
            return ""

        lines = content.split("\n")
        cleaned_lines = []

        # ì œì™¸í•  íŒ¨í„´ë“¤
        skip_patterns = [
            r"^ì¢‹ì•„ìš”\s*$",
            r"^\d+\s*$",  # ìˆ«ìë§Œ ìˆëŠ” ì¤„
            r"^ê´€ë ¨ê¸°ì‚¬\s*$",
            r"^ë‹¤ë¥¸ê¸°ì‚¬\s*ë³´ê¸°",
            r"^í‚¤ì›Œë“œ\s*$",
            r"^#\w+",  # í•´ì‹œíƒœê·¸
            r"^ì €ì‘ê¶Œì",
            r"ë¬´ë‹¨ì „ì¬",
            r"ì¬ë°°í¬.*ê¸ˆì§€",
            r"^ê¸°ì$",
            r"@.*\.com",  # ì´ë©”ì¼
            r"^news@",
            r"^\S+ê¸°ì$",
            r"^â–¶",  # ê´€ë ¨ ê¸°ì‚¬ ë§í¬
            r"^â˜",
            r"^\[ê´€ë ¨ê¸°ì‚¬\]",
            r"^\[.*ê¸°ì\]$",
            r"^ì‚¬ì§„=",
            r"^\(ì‚¬ì§„=",
            r"^ì¶œì²˜=",
            r"^\(ì¶œì²˜=",
            r"^â“’",
            r"^Â©",
            r"^Copyrights",
            r"AIí•™ìŠµ.*ê¸ˆì§€",
            r"ë‰´ìŠ¤ì œê³µ",
        ]

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ë„ˆë¬´ ì§§ì€ ì¤„ ì œì™¸ (3ì ë¯¸ë§Œ)
            if len(line) < 3:
                continue

            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì œì™¸
            should_skip = False
            for pattern in skip_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    should_skip = True
                    break

            if should_skip:
                continue

            cleaned_lines.append(line)

        return "\n\n".join(cleaned_lines)

    def _extract_main_image(self, soup, base_url: str) -> str:
        """ê¸°ì‚¬ì˜ ëŒ€í‘œ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        from urllib.parse import urljoin

        # ì´ë¯¸ì§€ ì„ íƒì (ìš°ì„ ìˆœìœ„ ìˆœ)
        image_selectors = [
            # Open Graph ì´ë¯¸ì§€ (ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆìŒ)
            'meta[property="og:image"]',
            # Twitter ì¹´ë“œ ì´ë¯¸ì§€
            'meta[name="twitter:image"]',
            # ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ ì²« ë²ˆì§¸ ì´ë¯¸ì§€
            "article img",
            "#article-view-content-div img",
            ".article-body img",
            ".article_body img",
            ".article-content img",
            'div[itemprop="articleBody"] img',
        ]

        for selector in image_selectors:
            element = soup.select_one(selector)
            if element:
                # meta íƒœê·¸ì¸ ê²½ìš°
                if element.name == "meta":
                    image_url = element.get("content")
                # img íƒœê·¸ì¸ ê²½ìš°
                else:
                    image_url = element.get("src") or element.get("data-src")

                if image_url:
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    image_url = urljoin(base_url, image_url)

                    # ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸ (ê¸°ë³¸ì ì¸ í•„í„°ë§)
                    if self._is_valid_image_url(image_url):
                        return image_url

        return None

    def _extract_all_images(self, soup, base_url: str) -> list:
        """ê¸°ì‚¬ì˜ ëª¨ë“  ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        from urllib.parse import urljoin

        images = []
        seen_urls = set()

        # 1. Open Graph ì´ë¯¸ì§€ ë¨¼ì € ì¶”ê°€
        og_image = soup.select_one('meta[property="og:image"]')
        if og_image:
            url = og_image.get("content")
            if url and self._is_valid_image_url(url):
                full_url = urljoin(base_url, url)
                if full_url not in seen_urls:
                    images.append(full_url)
                    seen_urls.add(full_url)

        # 2. ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ ëª¨ë“  ì´ë¯¸ì§€
        article_selectors = [
            "#article-view-content-div img",
            "article img",
            ".article-body img",
            ".article_body img",
            ".article-content img",
            'div[itemprop="articleBody"] img',
        ]

        for selector in article_selectors:
            for img in soup.select(selector):
                url = img.get("src") or img.get("data-src") or img.get("data-original")
                if url:
                    full_url = urljoin(base_url, url)
                    if full_url not in seen_urls and self._is_valid_image_url(full_url):
                        images.append(full_url)
                        seen_urls.add(full_url)

        # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ (ë„ˆë¬´ ë§ìœ¼ë©´ í˜ì´ì§€ê°€ ë¬´ê±°ì›Œì§)
        return images[:10]

    def _is_valid_image_url(self, url: str) -> bool:
        """ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸"""
        if not url:
            return False

        # ê´‘ê³ /íŠ¸ë˜í‚¹ ì´ë¯¸ì§€ ì œì™¸
        exclude_patterns = [
            "pixel",
            "tracking",
            "analytics",
            "beacon",
            "advertisement",
            "banner",
            "ad.",
            "ads.",
            "1x1",
            "spacer",
            "blank",
            "transparent",
        ]

        url_lower = url.lower()
        for pattern in exclude_patterns:
            if pattern in url_lower:
                return False

        # ì´ë¯¸ì§€ í™•ì¥ì ë˜ëŠ” ì´ë¯¸ì§€ ì„œë¹„ìŠ¤ í™•ì¸
        valid_extensions = [".jpg", ".jpeg", ".png", ".gif", ".webp"]
        has_valid_ext = any(ext in url_lower for ext in valid_extensions)

        # ì´ë¯¸ì§€ ì„œë¹„ìŠ¤ URL (í™•ì¥ì ì—†ì´ ì´ë¯¸ì§€ ì œê³µ)
        image_services = ["wp-content/uploads", "images", "img", "photo", "media"]
        is_image_service = any(svc in url_lower for svc in image_services)

        return has_valid_ext or is_image_service


# =============================================================================
# ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
# =============================================================================


class MarkdownArchive:
    """ë‰´ìŠ¤ë¥¼ ì›”ë³„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥"""

    def __init__(self, base_dir: str = None):
        """
        Args:
            base_dir: ì €ì¥ ê¸°ë³¸ ê²½ë¡œ. Noneì´ë©´ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ì‚¬ìš©
        """
        if base_dir:
            self.base_dir = base_dir
        else:
            self.base_dir = os.path.dirname(os.path.abspath(__file__))

    def save_news(self, news: dict, analysis: dict) -> bool:
        """
        ë‰´ìŠ¤ë¥¼ ì¼ë³„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì— ì €ì¥

        Args:
            news: ë‰´ìŠ¤ ë°ì´í„° (title, link, content, date, source)
            analysis: ë¶„ì„ ê²°ê³¼ (summary, technologies, organization, importance, key_points)

        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€ (ì¤‘ë³µì´ë©´ False)
        """
        # ë‚ ì§œ íŒŒì‹±
        try:
            news_date = datetime.strptime(news["date"], "%Y-%m-%d")
        except:
            news_date = datetime.now()

        year = str(news_date.year)
        month = f"{news_date.month:02d}ì›”"
        day = f"{news_date.month:02d}-{news_date.day:02d}"

        # í´ë” ìƒì„±: ì—°ë„/ì›”/
        month_dir = os.path.join(self.base_dir, year, month)
        os.makedirs(month_dir, exist_ok=True)

        # íŒŒì¼ ê²½ë¡œ: ì—°ë„/ì›”/MM-DD.md
        md_file = os.path.join(month_dir, f"{day}.md")

        # ì¤‘ë³µ ì²´í¬
        if self._is_duplicate(md_file, news["title"], news["link"]):
            return False

        # ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ìƒì„±
        md_content = self._format_news(news, analysis)

        # ì¼ë³„ íŒŒì¼ì— ì¶”ê°€
        self._append_to_file(md_file, md_content, news_date)

        # ì›” ì´ê´„ íŒŒì¼ ì—…ë°ì´íŠ¸
        self._update_monthly_index(month_dir, news_date)

        return True

    def regenerate_all_indexes(self):
        """ëª¨ë“  ì›”ë³„ README.md ì¬ìƒì„±"""
        regenerated = 0

        # ì—°ë„ í´ë” íƒìƒ‰
        for year_dir in os.listdir(self.base_dir):
            year_path = os.path.join(self.base_dir, year_dir)
            if not os.path.isdir(year_path) or not year_dir.isdigit():
                continue

            # ì›” í´ë” íƒìƒ‰
            for month_dir in os.listdir(year_path):
                month_path = os.path.join(year_path, month_dir)
                if not os.path.isdir(month_path) or "ì›”" not in month_dir:
                    continue

                # ì¼ë³„ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                has_daily_files = any(
                    f.endswith(".md") and f != "README.md"
                    for f in os.listdir(month_path)
                )

                if has_daily_files:
                    # ì„ì˜ì˜ ë‚ ì§œë¡œ ì›” ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (ì›” ì •ë³´ë§Œ í•„ìš”)
                    month_num = int(month_dir.replace("ì›”", ""))
                    dummy_date = datetime(int(year_dir), month_num, 1)
                    self._update_monthly_index(month_path, dummy_date)
                    print(f"âœ… ì¬ìƒì„±: {year_dir}/{month_dir}/README.md")
                    regenerated += 1

        return regenerated

    def _update_monthly_index(self, month_dir: str, news_date: datetime):
        """ì›” ì´ê´„ íŒŒì¼(README.md) ì—…ë°ì´íŠ¸"""
        import re

        index_file = os.path.join(month_dir, "README.md")
        month_title = news_date.strftime("%Yë…„ %mì›”")

        # í•´ë‹¹ ì›”ì˜ ëª¨ë“  ì¼ë³„ íŒŒì¼ ìˆ˜ì§‘
        daily_files = []
        for filename in sorted(os.listdir(month_dir), reverse=True):  # ìµœì‹ ìˆœ
            if filename.endswith(".md") and filename != "README.md":
                daily_files.append(filename)

        # ê° ì¼ë³„ íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
        toc_content = []
        total_count = 0

        for daily_file in daily_files:
            filepath = os.path.join(month_dir, daily_file)
            day_name = daily_file.replace(".md", "")  # "12-27"

            # ë‚ ì§œ íŒŒì‹±í•´ì„œ ë³´ê¸° ì¢‹ê²Œ
            try:
                month_num, day_num = day_name.split("-")
                display_date = f"{int(month_num)}ì›” {int(day_num)}ì¼"
            except:
                display_date = day_name

            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()

            # ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ (### ë’¤ì— ì˜¤ëŠ” ì œëª©, ë‹¨ ğŸ“‘ ëª©ì°¨ ì œì™¸)
            news_titles = re.findall(r"^### (?!ğŸ“‘)(.+)", content, re.MULTILINE)
            news_count = len(news_titles)
            total_count += news_count

            # ì¼ë³„ ì„¹ì…˜ ì¶”ê°€
            toc_content.append(f"\n### ğŸ“… {display_date} ({news_count}ê±´)")
            toc_content.append(f"ğŸ“„ [{day_name}.md](./{daily_file})\n")

            for title in news_titles:
                display_title = title[:50] + "..." if len(title) > 50 else title
                toc_content.append(f"- {display_title}")

        # README.md ìƒì„±
        readme_content = f"""# ğŸ¤– AI ë‰´ìŠ¤ ì•„ì¹´ì´ë¸Œ - {month_title}

> ì´ **{total_count}ê±´**ì˜ ë‰´ìŠ¤ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ“‘ ëª©ì°¨

{''.join(chr(10) + line for line in toc_content)}

---

*ì´ íŒŒì¼ì€ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.*
"""

        with open(index_file, "w", encoding="utf-8") as f:
            f.write(readme_content)

    def _is_duplicate(self, filepath: str, title: str, link: str) -> bool:
        """íŒŒì¼ì—ì„œ ì¤‘ë³µ ë‰´ìŠ¤ ì²´í¬"""
        if not os.path.exists(filepath):
            return False

        try:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
                # ì œëª©ì´ë‚˜ ë§í¬ë¡œ ì¤‘ë³µ í™•ì¸
                if title in content or link in content:
                    return True
        except:
            pass

        return False

    def _format_news(self, news: dict, analysis: dict) -> str:
        """ë‰´ìŠ¤ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        lines = []

        # ì œëª© (ì œëª©ì—ì„œ ë‚ ì§œ íƒœê·¸ ì œê±°í•˜ì—¬ ê¹”ë”í•˜ê²Œ)
        import re

        clean_title = re.sub(
            r"^\[\d{1,2}ì›”\d{1,2}ì¼\]\s*", "", news["title"]
        )  # [12ì›”26ì¼] í˜•ì‹ ì œê±°
        clean_title = re.sub(
            r"^\[\d{4}\.\d{2}\.\d{2}\]\s*", "", clean_title
        )  # [2025.12.26] í˜•ì‹ ì œê±°

        lines.append(f"### {clean_title}")
        lines.append("")

        # ë©”íƒ€ ì •ë³´ (ë°œí–‰ì¼ ì¶”ê°€)
        importance = analysis.get("importance", "ğŸ“Œ ì¼ë°˜")
        org = analysis.get("organization", "ê¸°íƒ€")
        techs = ", ".join(analysis.get("technologies", []))

        lines.append(
            f"> ğŸ“… **{news['date']}** | **{importance}** | {org} | {news['source']}"
        )
        if techs:
            lines.append(f"> ğŸ·ï¸ {techs}")
        lines.append("")

        # ìš”ì•½
        summary = analysis.get("summary", "")
        if summary:
            lines.append("**ğŸ’¡ ìš”ì•½**")
            lines.append(f"{summary}")
            lines.append("")

        # í•µì‹¬ í¬ì¸íŠ¸
        key_points = analysis.get("key_points", [])
        if key_points:
            lines.append("**ğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸**")
            for point in key_points[:5]:
                lines.append(f"- {point}")
            lines.append("")

        # ì›ë¬¸ ë‚´ìš© (ìµœëŒ€ 1000ì)
        content = news.get("content", "")
        if content:
            lines.append("<details>")
            lines.append("<summary><b>ğŸ“„ ì›ë¬¸ ë³´ê¸°</b></summary>")
            lines.append("")
            # ì›ë¬¸ ì •ë¦¬ (ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°)
            clean_content = content[:1500].strip()
            if len(content) > 1500:
                clean_content += "..."
            lines.append(clean_content)
            lines.append("")
            lines.append("</details>")
            lines.append("")

        # ì¶œì²˜ ë§í¬
        lines.append(f"ğŸ”— [ì›ë¬¸ ë³´ê¸°]({news['link']})")
        lines.append("")
        lines.append("---")
        lines.append("")

        return "\n".join(lines)

    def _append_to_file(self, filepath: str, content: str, news_date: datetime):
        """íŒŒì¼ì— ë‚´ìš© ì¶”ê°€ (ëª©ì°¨ í¬í•¨)"""
        date_title = news_date.strftime("%Yë…„ %mì›” %dì¼")

        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if not os.path.exists(filepath):
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(f"# ğŸ¤– AI ë‰´ìŠ¤ - {date_title}\n\n")
                f.write("## ğŸ“‘ ëª©ì°¨\n\n")
                f.write("<!-- TOC_START -->\n")
                f.write("<!-- TOC_END -->\n\n")
                f.write("---\n\n")
                f.write(content)
            self._update_toc(filepath)
            return

        # ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€
        with open(filepath, "r", encoding="utf-8") as f:
            existing = f.read()

        # êµ¬ë¶„ì„ (---) ë’¤ì— ìƒˆ ì½˜í…ì¸  ì¶”ê°€
        # ë§ˆì§€ë§‰ --- ì°¾ì•„ì„œ ê·¸ ë’¤ì— ì¶”ê°€
        new_content = existing.rstrip() + "\n\n" + content

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(new_content)

        # ëª©ì°¨ ì—…ë°ì´íŠ¸
        self._update_toc(filepath)

    def _update_toc(self, filepath: str):
        """íŒŒì¼ì˜ ëª©ì°¨ë¥¼ ì—…ë°ì´íŠ¸"""
        import re

        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()

        # ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
        toc_entries = []

        # ë‰´ìŠ¤ ì œëª© ì°¾ê¸° (### ì œëª© í˜•ì‹)
        news_pattern = r"### ([^#\n].+)"

        lines = content.split("\n")
        for line in lines:
            news_match = re.match(news_pattern, line)
            if news_match:
                title = news_match.group(1)
                # ì•µì»¤ ìƒì„±
                anchor = self._create_anchor(title)
                toc_entries.append({"title": title, "anchor": anchor})

        # ëª©ì°¨ ìƒì„±
        toc_lines = []
        for i, entry in enumerate(toc_entries, 1):
            # ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
            display_title = (
                entry["title"][:60] + "..."
                if len(entry["title"]) > 60
                else entry["title"]
            )
            toc_lines.append(f"{i}. [{display_title}](#{entry['anchor']})")

        toc_content = "\n".join(toc_lines) if toc_lines else "(ë‰´ìŠ¤ ì—†ìŒ)"

        # ëª©ì°¨ ì˜ì—­ êµì²´
        new_content = re.sub(
            r"<!-- TOC_START -->.*?<!-- TOC_END -->",
            f"<!-- TOC_START -->\n{toc_content}\n<!-- TOC_END -->",
            content,
            flags=re.DOTALL,
        )

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(new_content)

    def _create_anchor(self, title: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ ì•µì»¤ ìƒì„± (GitHub ìŠ¤íƒ€ì¼)"""
        import re

        # ì†Œë¬¸ì ë³€í™˜
        anchor = title.lower()
        # ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, í•˜ì´í”ˆë§Œ ìœ ì§€)
        anchor = re.sub(r"[^\w\sê°€-í£-]", "", anchor)
        # ê³µë°±ì„ í•˜ì´í”ˆìœ¼ë¡œ
        anchor = re.sub(r"\s+", "-", anchor)
        anchor = anchor.strip("-")
        return anchor


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================


class AINewsBot:
    """AI ë‰´ìŠ¤ ìë™í™” ë´‡"""

    def __init__(self, archive_dir: str = None, provider: str = "openai"):
        """
        Args:
            archive_dir: ë§ˆí¬ë‹¤ìš´ ì•„ì¹´ì´ë¸Œ ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜)
            provider: AI ì œê³µì - "openai" (ê¸°ë³¸) ë˜ëŠ” "claude"
        """
        self.notion = NotionClient(NOTION_API_KEY)
        self.collector = NewsCollector(RSS_FEEDS)
        self.archive = MarkdownArchive(archive_dir)
        self.provider = provider.lower()

        # API í‚¤ ì„¤ì •
        if self.provider == "claude":
            if not ANTHROPIC_API_KEY:
                raise ValueError("ANTHROPIC_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.analyzer = NewsAnalyzer(ANTHROPIC_API_KEY, provider="claude")
            print(f"ğŸ¤– Claude API ì‚¬ìš© (ëª¨ë¸: claude-sonnet-4-20250514)")
        else:
            if not OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.analyzer = NewsAnalyzer(OPENAI_API_KEY, provider="openai")
            print(f"ğŸ¤– OpenAI API ì‚¬ìš© (ëª¨ë¸: gpt-5-nano) - ğŸ’° ìµœì €ê°€!")

    def run(self, days: int = 1, use_ai: bool = True, no_notion: bool = False):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì—…ë¡œë“œ ì‹¤í–‰

        Args:
            days: ìˆ˜ì§‘í•  ê¸°ê°„ (ì¼)
            use_ai: AI API ì‚¬ìš© ì—¬ë¶€
            no_notion: Trueë©´ Notion ì—…ë¡œë“œ ê±´ë„ˆë›°ê¸°
        """
        print(f"ğŸ” ìµœê·¼ {days}ì¼ AI ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        if no_notion:
            print("ğŸ“ Notion ì—…ë¡œë“œ ë¹„í™œì„±í™” - ë§ˆí¬ë‹¤ìš´ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")

        # ë‰´ìŠ¤ ìˆ˜ì§‘
        news_list = self.collector.collect_news(days=days)
        print(f"ğŸ“° {len(news_list)}ê°œ ë‰´ìŠ¤ ë°œê²¬")

        uploaded = 0
        skipped = 0
        filtered = 0
        md_saved = 0
        saved_dates = set()  # ì €ì¥ëœ ë‚ ì§œë“¤ ìˆ˜ì§‘

        for news in news_list:
            # ì¤‘ë³µ ì²´í¬ (Notion) - no_notion ëª¨ë“œì—ì„œëŠ” ê±´ë„ˆë›°ê¸°
            if not no_notion:
                notion_duplicate = self.notion.check_duplicate(DATABASE_ID, news["title"])
                if notion_duplicate:
                    print(f"â­ï¸ ì¤‘ë³µ ê±´ë„ˆë›°ê¸°: {news['title'][:30]}...")
                    skipped += 1
                    continue

            # ë‰´ìŠ¤ ë¶„ì„
            if use_ai:
                analysis = self.analyzer.analyze_news(news["title"], news["content"])
            else:
                analysis = self.analyzer._fallback_analysis(
                    news["title"], news["content"]
                )

            # AI ê´€ë ¨ì„± í•„í„°
            if not analysis.get("is_ai_related", True):
                reason = analysis.get("rejection_reason", "AI ë¹„ê´€ë ¨")
                print(f"ğŸš« AI ë¹„ê´€ë ¨ ì œì™¸: {news['title'][:30]}... ({reason})")
                filtered += 1
                continue

            # ë‚ ì§œì—ì„œ ì—°ë„/ì›” ì¶”ì¶œ
            try:
                news_date = datetime.strptime(news["date"], "%Y-%m-%d")
                year = str(news_date.year)
                month = f"{news_date.month:02d}ì›”"
            except:
                year = str(datetime.now().year)
                month = f"{datetime.now().month:02d}ì›”"

            # Notion ì†ì„± êµ¬ì„±
            properties = {
                "ì œëª©": {"title": [{"text": {"content": news["title"][:100]}}]},
                "ë‚ ì§œ": {"date": {"start": news["date"]}},
                "ì—°ë„": {"select": {"name": year}},
                "ì›”": {"select": {"name": month}},
                "ì¶œì²˜": {"url": news["link"]},
                "ìš”ì•½": {
                    "rich_text": [
                        {"text": {"content": analysis.get("summary", "")[:200]}}
                    ]
                },
                "ê´€ë ¨ ê¸°ìˆ ": {
                    "multi_select": [
                        {"name": tech} for tech in analysis.get("technologies", [])[:5]
                    ]
                },
                "ê¸°ì—…/ê¸°ê´€": {"select": {"name": analysis.get("organization", "ê¸°íƒ€")}},
                "ì¤‘ìš”ë„": {"select": {"name": analysis.get("importance", "ğŸ“Œ ì¼ë°˜")}},
            }

            # Notionì— ì—…ë¡œë“œ (no_notion ëª¨ë“œì—ì„œëŠ” ê±´ë„ˆë›°ê¸°)
            if not no_notion:
                try:
                    # í˜ì´ì§€ ë‚´ìš©ì— ì‚¬ìš©í•  ë°ì´í„°
                    page_content = {
                        "summary": analysis.get("summary", ""),
                        "key_sentences": analysis.get(
                            "key_sentences", []
                        ),  # í•µì‹¬ ë¬¸ì¥ (1~5ê°œ)
                        "image_url": news.get("image_url"),
                        "all_images": news.get("all_images", []),
                        "link": news["link"],
                        "date": news["date"],
                        "source": news["source"],
                    }

                    result = self.notion.create_page(DATABASE_ID, properties, page_content)
                    if "id" in result:
                        img_icon = "ğŸ–¼ï¸" if news.get("image_url") else "ğŸ“„"
                        print(f"âœ… {img_icon} Notion ì—…ë¡œë“œ ì™„ë£Œ: {news['title'][:40]}...")
                        uploaded += 1
                    else:
                        print(
                            f"âŒ Notion ì—…ë¡œë“œ ì‹¤íŒ¨: {result.get('message', 'Unknown error')}"
                        )
                except Exception as e:
                    print(f"âŒ Notion ì˜¤ë¥˜: {e}")

            # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì— ì €ì¥
            try:
                if self.archive.save_news(news, analysis):
                    print(f"ğŸ“ ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì™„ë£Œ: {news['title'][:40]}...")
                    md_saved += 1
                    # ì €ì¥ëœ ë‚ ì§œ ìˆ˜ì§‘ (MM/DD í˜•ì‹)
                    try:
                        news_date = datetime.strptime(news["date"], "%Y-%m-%d")
                        saved_dates.add(f"{news_date.month}/{news_date.day}")
                    except:
                        pass
                else:
                    print(f"â­ï¸ ë§ˆí¬ë‹¤ìš´ ì¤‘ë³µ ê±´ë„ˆë›°ê¸°: {news['title'][:30]}...")
            except Exception as e:
                print(f"âŒ ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì˜¤ë¥˜: {e}")

        print(f"\nğŸ“Š ì™„ë£Œ!")
        print(f"   - Notion ì—…ë¡œë“œ: {uploaded}ê°œ")
        print(f"   - ë§ˆí¬ë‹¤ìš´ ì €ì¥: {md_saved}ê°œ")
        print(f"   - AI ë¹„ê´€ë ¨ ì œì™¸: {filtered}ê°œ")
        print(f"   - ì¤‘ë³µ ê±´ë„ˆë›°ê¸°: {skipped}ê°œ")

        # ê²°ê³¼ ë°˜í™˜: (ì—…ë¡œë“œ ìˆ˜, ë§ˆí¬ë‹¤ìš´ ì €ì¥ ìˆ˜, ì €ì¥ëœ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸)
        return {
            "uploaded": uploaded,
            "md_saved": md_saved,
            "filtered": filtered,
            "skipped": skipped,
            "saved_dates": sorted(saved_dates),  # ì •ë ¬ëœ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸
        }


# =============================================================================
# ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="AI ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ê¸°")
    parser.add_argument("--days", type=int, default=1, help="ìˆ˜ì§‘í•  ê¸°ê°„ (ì¼)")
    parser.add_argument(
        "--no-ai", action="store_true", help="AI API ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í‚¤ì›Œë“œ ê¸°ë°˜)"
    )
    parser.add_argument(
        "--provider",
        type=str,
        default="openai",
        choices=["openai", "claude"],
        help="AI ì œê³µì ì„ íƒ (ê¸°ë³¸: openai - gpt-5-nano)",
    )
    parser.add_argument(
        "--archive-dir", type=str, default=None, help="ë§ˆí¬ë‹¤ìš´ ì•„ì¹´ì´ë¸Œ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--no-notion", action="store_true", help="Notion ì—…ë¡œë“œ ë¹„í™œì„±í™” (ë§ˆí¬ë‹¤ìš´ë§Œ ì €ì¥)"
    )
    parser.add_argument(
        "--regenerate-index", action="store_true", help="ëª¨ë“  ì›”ë³„ README.md ì¬ìƒì„±"
    )

    args = parser.parse_args()

    # --regenerate-index ëª¨ë“œ
    if args.regenerate_index:
        print("ğŸ”„ ì›”ë³„ README.md ì¬ìƒì„± ì¤‘...")
        archive = MarkdownArchive()
        count = archive.regenerate_all_indexes()
        print(f"\nâœ… {count}ê°œì˜ README.md íŒŒì¼ì´ ì¬ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        exit(0)

    # API í‚¤ í™•ì¸ (no_notion ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ Notion API í‚¤ í•„ìš”)
    if not args.no_notion and not NOTION_API_KEY:
        print("âŒ NOTION_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        exit(1)

    if not args.no_ai:
        if args.provider == "openai" and not OPENAI_API_KEY:
            print("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            exit(1)
        elif args.provider == "claude" and not ANTHROPIC_API_KEY:
            print("âŒ ANTHROPIC_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            exit(1)

    bot = AINewsBot(archive_dir=args.archive_dir, provider=args.provider)
    bot.run(days=args.days, use_ai=not args.no_ai, no_notion=args.no_notion)
