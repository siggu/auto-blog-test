"""
AI ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ ë° Notion ë°ì´í„°ë² ì´ìŠ¤ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ì›¹ì—ì„œ ìµœì‹  AI ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘ (RSS í”¼ë“œ ë˜ëŠ” ì›¹ ìŠ¤í¬ë˜í•‘)
2. Claude APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ë¶„ì„ ë° ë¶„ë¥˜
3. Notion ë°ì´í„°ë² ì´ìŠ¤ì— ìë™ ì—…ë¡œë“œ

ì‚¬ìš© ì „ ì„¤ì •:
1. .env íŒŒì¼ì— API í‚¤ ì„¤ì •
2. í¬ë¡ ì¡ ë˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ì •ê¸° ì‹¤í–‰ ì„¤ì •
"""

import os
import json
import requests
from datetime import datetime, timedelta
from typing import Optional
import feedparser
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# =============================================================================
# ì„¤ì •
# =============================================================================

NOTION_API_KEY = os.getenv("NOTION_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

# Notion ë°ì´í„°ë² ì´ìŠ¤ ID (ìƒì„±ëœ ë°ì´í„°ë² ì´ìŠ¤)
DATABASE_ID = "3e6b5982ea584534afa6618150f29d21"

# AI ë‰´ìŠ¤ RSS í”¼ë“œ ëª©ë¡
RSS_FEEDS = [
    {
        "name": "AIíƒ€ì„ìŠ¤",
        "url": "https://www.aitimes.com/rss/allArticle.xml",
        "language": "ko",
    },
    {
        "name": "ì¸ê³µì§€ëŠ¥ì‹ ë¬¸",
        "url": "https://www.aitimes.kr/rss/allArticle.xml",
        "language": "ko",
    },
    {
        "name": "MIT Tech Review AI",
        "url": "https://www.technologyreview.com/topic/artificial-intelligence/feed",
        "language": "en",
    },
    {
        "name": "VentureBeat AI",
        "url": "https://venturebeat.com/category/ai/feed/",
        "language": "en",
    },
    {
        "name": "The Verge AI",
        "url": "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml",
        "language": "en",
    },
]

# ê´€ë ¨ ê¸°ìˆ  í‚¤ì›Œë“œ ë§¤í•‘
TECH_KEYWORDS = {
    "LLM": [
        "llm",
        "large language model",
        "gpt",
        "claude",
        "gemini",
        "ëŒ€í˜•ì–¸ì–´ëª¨ë¸",
        "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸",
        "chatgpt",
    ],
    "ì´ë¯¸ì§€ ìƒì„±": [
        "image generation",
        "dall-e",
        "midjourney",
        "stable diffusion",
        "ì´ë¯¸ì§€ ìƒì„±",
        "ê·¸ë¦¼ ìƒì„±",
        "text-to-image",
    ],
    "ì¶”ë¡  AI": [
        "reasoning",
        "o1",
        "o3",
        "chain of thought",
        "ì¶”ë¡ ",
        "ì‚¬ê³ ",
        "thinking",
    ],
    "ì—ì´ì „íŠ¸": ["agent", "agentic", "ì—ì´ì „íŠ¸", "ììœ¨ ì—ì´ì „íŠ¸", "autonomous"],
    "ë©€í‹°ëª¨ë‹¬": ["multimodal", "vision", "audio", "ë©€í‹°ëª¨ë‹¬", "ë‹¤ì¤‘ëª¨ë‹¬", "ë¹„ì „"],
    "ì˜¤í”ˆì†ŒìŠ¤": ["open source", "ì˜¤í”ˆì†ŒìŠ¤", "opensource", "hugging face", "í—ˆê¹…í˜ì´ìŠ¤"],
    "ê°•í™”í•™ìŠµ": ["reinforcement learning", "rl", "rlhf", "ê°•í™”í•™ìŠµ", "ë³´ìƒ ëª¨ë¸"],
    "ë¡œë³´í‹±ìŠ¤": ["robot", "robotics", "ë¡œë´‡", "ë¡œë³´í‹±ìŠ¤", "embodied ai"],
    "ìŒì„±/ì˜¤ë””ì˜¤": [
        "voice",
        "audio",
        "speech",
        "tts",
        "stt",
        "ìŒì„±",
        "ì˜¤ë””ì˜¤",
        "whisper",
    ],
}

# ê¸°ì—…/ê¸°ê´€ í‚¤ì›Œë“œ ë§¤í•‘
ORG_KEYWORDS = {
    "OpenAI": [
        "openai",
        "chatgpt",
        "gpt-4",
        "gpt-5",
        "dall-e",
        "sam altman",
        "ìƒ˜ ì˜¬íŠ¸ë¨¼",
    ],
    "Google": ["google", "êµ¬ê¸€", "deepmind", "ë”¥ë§ˆì¸ë“œ", "gemini", "ì œë¯¸ë‚˜ì´", "bard"],
    "Anthropic": ["anthropic", "ì•¤ìŠ¤ë¡œí”½", "claude", "í´ë¡œë“œ"],
    "Meta": ["meta", "ë©”íƒ€", "facebook", "llama", "ë¼ë§ˆ"],
    "Microsoft": ["microsoft", "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "copilot", "ì½”íŒŒì¼ëŸ¿", "azure"],
    "NVIDIA": ["nvidia", "ì—”ë¹„ë””ì•„", "cuda", "gpu", "h100", "blackwell"],
    "êµ­ë‚´ ì—°êµ¬ê¸°ê´€": [
        "kaist",
        "ì¹´ì´ìŠ¤íŠ¸",
        "ì„œìš¸ëŒ€",
        "postech",
        "í¬ìŠ¤í…",
        "unist",
        "etri",
        "í•œêµ­ì „ìí†µì‹ ì—°êµ¬ì›",
    ],
}


# =============================================================================
# Notion API í´ë¼ì´ì–¸íŠ¸
# =============================================================================


class NotionClient:
    """Notion API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.notion.com/v1"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28",
        }

    def create_page(
        self, database_id: str, properties: dict, news_data: dict = None
    ) -> dict:
        """ë°ì´í„°ë² ì´ìŠ¤ì— ìƒˆ í˜ì´ì§€ ìƒì„± (ì›ë¬¸ ë³´ì¡´)"""
        url = f"{self.base_url}/pages"

        data = {"parent": {"database_id": database_id}, "properties": properties}

        # ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í˜ì´ì§€ ë‚´ìš© êµ¬ì„±
        if news_data:
            children = []

            # 1. ìš”ì•½ ì„¹ì…˜ (AI ë¶„ì„ ê²°ê³¼)
            children.append(
                {
                    "object": "block",
                    "type": "callout",
                    "callout": {
                        "rich_text": [
                            {
                                "type": "text",
                                "text": {
                                    "content": news_data.get("summary", "ìš”ì•½ ì—†ìŒ")
                                },
                            }
                        ],
                        "icon": {"emoji": "ğŸ’¡"},
                        "color": "blue_background",
                    },
                }
            )

            # 2. í•µì‹¬ í¬ì¸íŠ¸ (ìˆëŠ” ê²½ìš°)
            key_points = news_data.get("key_points", [])
            if key_points:
                children.append(
                    {
                        "object": "block",
                        "type": "heading_2",
                        "heading_2": {
                            "rich_text": [
                                {"type": "text", "text": {"content": "ğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸"}}
                            ]
                        },
                    }
                )
                for point in key_points[:5]:
                    children.append(
                        {
                            "object": "block",
                            "type": "bulleted_list_item",
                            "bulleted_list_item": {
                                "rich_text": [
                                    {"type": "text", "text": {"content": point}}
                                ]
                            },
                        }
                    )

            # êµ¬ë¶„ì„ 
            children.append({"object": "block", "type": "divider", "divider": {}})

            # 3. ì›ë¬¸ ë‚´ìš© (ìˆ˜ì • ì—†ì´ ê·¸ëŒ€ë¡œ)
            children.append(
                {
                    "object": "block",
                    "type": "heading_2",
                    "heading_2": {
                        "rich_text": [
                            {"type": "text", "text": {"content": "ğŸ“° ì›ë¬¸ ë‚´ìš©"}}
                        ]
                    },
                }
            )

            # ì›ë¬¸ ë‚´ìš©ì„ ë‹¨ë½ë³„ë¡œ ë¶„ë¦¬ (ê°€ë…ì„± í–¥ìƒ)
            original_content = news_data.get("content", "")

            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‹¨ë½ êµ¬ë¶„ (ë§ˆì¹¨í‘œ+ê³µë°± ë˜ëŠ” ë‹¤ ê¸°ì¤€)
            paragraphs = self._split_into_paragraphs(original_content)

            for para in paragraphs:
                para = para.strip()
                if not para:
                    continue
                children.append(
                    {
                        "object": "block",
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": [
                                {"type": "text", "text": {"content": para[:2000]}}
                            ]
                        },
                    }
                )

            # êµ¬ë¶„ì„ 
            children.append({"object": "block", "type": "divider", "divider": {}})

            # 4. ì¶œì²˜ ì •ë³´
            children.append(
                {
                    "object": "block",
                    "type": "heading_2",
                    "heading_2": {
                        "rich_text": [{"type": "text", "text": {"content": "ğŸ”— ì¶œì²˜"}}]
                    },
                }
            )

            if news_data.get("link"):
                children.append(
                    {
                        "object": "block",
                        "type": "bookmark",
                        "bookmark": {"url": news_data.get("link", "")},
                    }
                )

            # 5. ë©”íƒ€ ì •ë³´
            children.append(
                {
                    "object": "block",
                    "type": "callout",
                    "callout": {
                        "rich_text": [
                            {
                                "type": "text",
                                "text": {
                                    "content": f"ğŸ“… ë°œí–‰ì¼: {news_data.get('date', 'N/A')} | ğŸ“° ì¶œì²˜: {news_data.get('source', 'N/A')}"
                                },
                            }
                        ],
                        "icon": {"emoji": "â„¹ï¸"},
                        "color": "gray_background",
                    },
                }
            )

            data["children"] = children

        response = requests.post(url, headers=self.headers, json=data)
        return response.json()

    def _split_into_paragraphs(self, text: str, sentences_per_para: int = 3) -> list:
        """ì›ë¬¸ì„ ë‹¨ë½ìœ¼ë¡œ ë¶„ë¦¬ (ë‚´ìš© ìˆ˜ì • ì—†ì´ ê°€ë…ì„±ë§Œ í–¥ìƒ)"""
        import re

        if not text:
            return []

        # ì´ë¯¸ ë‹¨ë½ êµ¬ë¶„ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if "\n\n" in text:
            return [p.strip() for p in text.split("\n\n") if p.strip()]

        if "\n" in text:
            return [p.strip() for p in text.split("\n") if p.strip()]

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ ë¶€í˜¸ ê³ ë ¤)
        sentences = re.split(r"(?<=[.!?ã€‚])\s+", text)

        # Nê°œ ë¬¸ì¥ì”© ë¬¶ì–´ì„œ ë‹¨ë½ ìƒì„±
        paragraphs = []
        current_para = []

        for sentence in sentences:
            current_para.append(sentence)
            if len(current_para) >= sentences_per_para:
                paragraphs.append(" ".join(current_para))
                current_para = []

        # ë‚¨ì€ ë¬¸ì¥ ì²˜ë¦¬
        if current_para:
            paragraphs.append(" ".join(current_para))

        return paragraphs

    def query_database(self, database_id: str, filter_obj: dict = None) -> dict:
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬"""
        url = f"{self.base_url}/databases/{database_id}/query"
        data = {}
        if filter_obj:
            data["filter"] = filter_obj

        response = requests.post(url, headers=self.headers, json=data)
        return response.json()

    def check_duplicate(self, database_id: str, title: str) -> bool:
        """ì¤‘ë³µ ê¸°ì‚¬ ì²´í¬"""
        filter_obj = {
            "property": "ì œëª©",
            "title": {"contains": title[:50]},  # ì œëª© ì¼ë¶€ë¡œ ê²€ìƒ‰
        }
        result = self.query_database(database_id, filter_obj)
        return len(result.get("results", [])) > 0


# =============================================================================
# ë‰´ìŠ¤ ë¶„ì„ê¸° (Claude API ì‚¬ìš©)
# =============================================================================


class NewsAnalyzer:
    """Claude APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ë¶„ì„"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.anthropic.com/v1/messages"

    def analyze_news(self, title: str, content: str) -> dict:
        """ë‰´ìŠ¤ ë¶„ì„ ë° ë¶„ë¥˜ (ì›ë¬¸ ë³´ì¡´)"""

        headers = {
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json",
        }

        prompt = f"""ë‹¤ìŒ AI ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {content[:3000]}

ì¤‘ìš”: ì›ë¬¸ì˜ ë‚´ìš©ì„ ì ˆëŒ€ ìˆ˜ì •, ì‚­ì œ, ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ë¶„ì„ë§Œ í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "summary": "ì›ë¬¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ 2-3ë¬¸ì¥ ìš”ì•½ (í•œêµ­ì–´)",
    "key_points": ["ê¸°ì‚¬ì—ì„œ ì§ì ‘ ì¶”ì¶œí•œ í•µì‹¬ í¬ì¸íŠ¸ 1", "í•µì‹¬ í¬ì¸íŠ¸ 2", "í•µì‹¬ í¬ì¸íŠ¸ 3"],
    "technologies": ["ê´€ë ¨ ê¸°ìˆ  ëª©ë¡ - LLM, ì´ë¯¸ì§€ ìƒì„±, ì¶”ë¡  AI, ì—ì´ì „íŠ¸, ë©€í‹°ëª¨ë‹¬, ì˜¤í”ˆì†ŒìŠ¤, ê°•í™”í•™ìŠµ, ë¡œë³´í‹±ìŠ¤, ìŒì„±/ì˜¤ë””ì˜¤ ì¤‘ ì„ íƒ"],
    "organization": "ì£¼ìš” ê¸°ì—…/ê¸°ê´€ - OpenAI, Google, Anthropic, Meta, Microsoft, NVIDIA, êµ­ë‚´ ì—°êµ¬ê¸°ê´€, ê¸°íƒ€ ì¤‘ ì„ íƒ",
    "importance": "ì¤‘ìš”ë„ - ğŸ”¥ ì£¼ìš”, ğŸ“Œ ì¼ë°˜, ğŸ“ ì°¸ê³  ì¤‘ ì„ íƒ"
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        data = {
            "model": "claude-sonnet-4-20250514",
            "max_tokens": 1000,
            "messages": [{"role": "user", "content": prompt}],
        }

        try:
            response = requests.post(self.base_url, headers=headers, json=data)

            # API ì˜¤ë¥˜ í™•ì¸
            if response.status_code != 200:
                print(f"API ì˜¤ë¥˜ ({response.status_code}): {response.text[:200]}")
                return self._fallback_analysis(title, content)

            result = response.json()

            if "content" in result and len(result["content"]) > 0:
                text = result["content"][0]["text"]

                # JSON ì¶”ì¶œ ì‹œë„ (ì—¬ëŸ¬ ë°©ë²•)
                import re

                # 1. ì§ì ‘ íŒŒì‹± ì‹œë„
                try:
                    return json.loads(text)
                except:
                    pass

                # 2. JSON ë¸”ë¡ ì¶”ì¶œ (```json ... ``` í˜•ì‹)
                json_match = re.search(r"```json\s*([\s\S]*?)\s*```", text)
                if json_match:
                    try:
                        return json.loads(json_match.group(1))
                    except:
                        pass

                # 3. ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” JSON ì°¾ê¸°
                json_match = re.search(r"\{[\s\S]*\}", text)
                if json_match:
                    try:
                        return json.loads(json_match.group(0))
                    except:
                        pass

                print(f"JSON ì¶”ì¶œ ì‹¤íŒ¨. ì‘ë‹µ: {text[:200]}...")
            else:
                print(f"API ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {result}")
        except Exception as e:
            print(f"ë¶„ì„ ì˜¤ë¥˜: {e}")

        # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        return self._fallback_analysis(title, content)

    def _fallback_analysis(self, title: str, content: str) -> dict:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ë¶„ì„"""
        text = (title + " " + content).lower()

        # ê¸°ìˆ  ë¶„ë¥˜
        technologies = []
        for tech, keywords in TECH_KEYWORDS.items():
            if any(kw in text for kw in keywords):
                technologies.append(tech)

        # ê¸°ê´€ ë¶„ë¥˜
        organization = "ê¸°íƒ€"
        for org, keywords in ORG_KEYWORDS.items():
            if any(kw in text for kw in keywords):
                organization = org
                break

        return {
            "summary": title,
            "technologies": technologies[:3] if technologies else ["LLM"],
            "organization": organization,
            "importance": "ğŸ“Œ ì¼ë°˜",
        }


# =============================================================================
# ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
# =============================================================================


class NewsCollector:
    """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""

    def __init__(self, feeds: list):
        self.feeds = feeds

    def collect_news(self, days: int = 1) -> list:
        """ìµœê·¼ Nì¼ ì´ë‚´ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        from datetime import timezone

        cutoff_date = datetime.now(timezone.utc).replace(
            tzinfo=None
        )  # UTC ê¸°ì¤€, naiveë¡œ ë³€í™˜
        cutoff_date = cutoff_date - timedelta(days=days)
        all_news = []

        for feed_info in self.feeds:
            try:
                # User-Agent í—¤ë” ì¶”ê°€ (ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œ í•„ìš”)
                feed = feedparser.parse(
                    feed_info["url"],
                    agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                )

                for entry in feed.entries[:10]:  # ê° í”¼ë“œì—ì„œ ìµœëŒ€ 10ê°œ
                    # ë‚ ì§œ íŒŒì‹± (ë‹¤ì–‘í•œ í•„ë“œ ì‹œë„)
                    pub_date = self._parse_date(entry)

                    # íƒ€ì„ì¡´ ì •ë³´ ì œê±° (naive datetimeìœ¼ë¡œ í†µì¼)
                    if pub_date.tzinfo is not None:
                        pub_date = pub_date.replace(tzinfo=None)

                    # ê¸°ê°„ í•„í„°
                    if pub_date < cutoff_date:
                        continue

                    news_item = {
                        "title": entry.get("title", ""),
                        "link": entry.get("link", ""),
                        "content": self._get_content(entry),
                        "date": pub_date.strftime("%Y-%m-%d"),
                        "source": feed_info["name"],
                    }
                    all_news.append(news_item)

                    # ë””ë²„ê·¸ ì¶œë ¥
                    print(f"ğŸ“… {news_item['title'][:50]}... -> {news_item['date']}")

            except Exception as e:
                print(f"í”¼ë“œ ìˆ˜ì§‘ ì˜¤ë¥˜ ({feed_info['name']}): {e}")

        return all_news

    def _parse_date(self, entry) -> datetime:
        """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±"""
        from email.utils import parsedate_to_datetime
        import re

        # 1. published ë¬¸ìì—´ ë¨¼ì € ì‹œë„ (í•œêµ­ RSS í”¼ë“œëŠ” ëŒ€ë¶€ë¶„ ì´ í˜•ì‹)
        if hasattr(entry, "published") and entry.published:
            parsed = self._parse_date_string(entry.published)
            if parsed:
                return parsed

        # 2. published_parsed ì‹œë„
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            try:
                return datetime(*entry.published_parsed[:6])
            except:
                pass

        # 3. updated ë¬¸ìì—´ íŒŒì‹± ì‹œë„
        if hasattr(entry, "updated") and entry.updated:
            parsed = self._parse_date_string(entry.updated)
            if parsed:
                return parsed

        # 4. updated_parsed ì‹œë„
        if hasattr(entry, "updated_parsed") and entry.updated_parsed:
            try:
                return datetime(*entry.updated_parsed[:6])
            except:
                pass

        # 5. dc:date ì‹œë„ (Dublin Core)
        if hasattr(entry, "dc_date") and entry.dc_date:
            parsed = self._parse_date_string(entry.dc_date)
            if parsed:
                return parsed

        # í´ë°±: í˜„ì¬ ì‹œê°„
        return datetime.now()

    def _parse_date_string(self, date_str: str) -> Optional[datetime]:
        """ë¬¸ìì—´ ë‚ ì§œ íŒŒì‹±"""
        from email.utils import parsedate_to_datetime
        import re

        if not date_str:
            return None

        date_str = date_str.strip()

        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì‹œë„ (í•œêµ­ í˜•ì‹ ìš°ì„ )
        date_formats = [
            "%Y-%m-%d %H:%M:%S",  # í•œêµ­ RSS í˜•ì‹: 2025-12-25 19:09:25
            "%Y-%m-%d %H:%M",  # 2025-12-25 19:09
            "%Y-%m-%d",  # 2025-12-25
            "%Y.%m.%d %H:%M:%S",  # í•œêµ­ í˜•ì‹: 2025.12.25 19:09:25
            "%Y.%m.%d %H:%M",  # 2025.12.25 19:09
            "%Y.%m.%d",  # 2025.12.25
            "%Y/%m/%d %H:%M:%S",  # 2025/12/25 19:09:25
            "%Y/%m/%d",  # 2025/12/25
            "%Y-%m-%dT%H:%M:%S%z",  # ISO 8601
            "%Y-%m-%dT%H:%M:%SZ",  # ISO 8601 UTC
            "%Y-%m-%dT%H:%M:%S",  # ISO 8601 no tz
            "%d %b %Y %H:%M:%S",  # 25 Dec 2025 19:09:25
            "%a, %d %b %Y %H:%M:%S",  # Wed, 25 Dec 2025 19:09:25
        ]

        for fmt in date_formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue

        # +0900 ê°™ì€ íƒ€ì„ì¡´ ì œê±° í›„ ì¬ì‹œë„
        clean_date = re.sub(r"[+-]\d{4}$", "", date_str)
        clean_date = re.sub(r"\s+\w{3,4}$", "", clean_date)  # KST, GMT ë“± ì œê±°
        clean_date = clean_date.strip()

        for fmt in date_formats:
            try:
                return datetime.strptime(clean_date, fmt)
            except ValueError:
                continue

        # RFC 2822 í˜•ì‹ ì‹œë„ (ì˜ˆ: "Wed, 25 Dec 2024 10:30:00 +0900")
        try:
            return parsedate_to_datetime(date_str)
        except:
            pass

        return None

    def _get_content(self, entry) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ - RSS ë‚´ìš© + ì›¹ ìŠ¤í¬ë˜í•‘"""
        # ë¨¼ì € RSSì—ì„œ ê¸°ë³¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        rss_content = ""

        if hasattr(entry, "content") and entry.content:
            if isinstance(entry.content, list) and len(entry.content) > 0:
                rss_content = entry.content[0].get("value", "")

        if not rss_content and hasattr(entry, "summary") and entry.summary:
            rss_content = entry.summary

        if not rss_content and hasattr(entry, "description") and entry.description:
            rss_content = entry.description

        # ê¸°ì‚¬ ë§í¬ì—ì„œ ì „ì²´ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì‹œë„
        link = entry.get("link", "")
        if link:
            full_content = self._scrape_article(link)
            if full_content and len(full_content) > len(rss_content):
                return full_content

        return rss_content

    def _scrape_article(self, url: str) -> str:
        """ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # HTML íŒŒì‹±
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(response.text, "html.parser")

            # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒìë“¤ ì‹œë„
            content = None

            # AIíƒ€ì„ìŠ¤, ì¸ê³µì§€ëŠ¥ì‹ ë¬¸ ë“± í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            selectors = [
                "article#article-view-content-div",  # AIíƒ€ì„ìŠ¤
                "div#article-view-content-div",  # AIíƒ€ì„ìŠ¤
                "div.article-body",  # ì¼ë°˜
                "div.article_body",  # ì¼ë°˜
                "div.article-content",  # ì¼ë°˜
                "div.news-content",  # ì¼ë°˜
                "div.view_cont",  # ì¼ë¶€ í•œêµ­ ì‚¬ì´íŠ¸
                "div#articleBody",  # ì¼ë¶€ ì‚¬ì´íŠ¸
                "div.entry-content",  # WordPress
                "article.post-content",  # ë¸”ë¡œê·¸
                'div[itemprop="articleBody"]',  # Schema.org
                "article",  # ì¼ë°˜ article íƒœê·¸
            ]

            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for tag in element.select(
                        "script, style, nav, footer, aside, .ad, .advertisement, .social-share"
                    ):
                        tag.decompose()

                    content = element.get_text(separator="\n", strip=True)
                    if content and len(content) > 200:
                        break

            if content:
                # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                return content[:5000]

        except ImportError:
            print("âš ï¸ BeautifulSoup ë¯¸ì„¤ì¹˜. pip install beautifulsoup4 ì‹¤í–‰ í•„ìš”")
        except Exception as e:
            # ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ë„˜ì–´ê°
            pass

        return ""


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================


class AINewsBot:
    """AI ë‰´ìŠ¤ ìë™í™” ë´‡"""

    def __init__(self):
        self.notion = NotionClient(NOTION_API_KEY)
        self.analyzer = NewsAnalyzer(ANTHROPIC_API_KEY)
        self.collector = NewsCollector(RSS_FEEDS)

    def run(self, days: int = 1, use_claude: bool = True):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì—…ë¡œë“œ ì‹¤í–‰"""
        print(f"ğŸ” ìµœê·¼ {days}ì¼ AI ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

        # ë‰´ìŠ¤ ìˆ˜ì§‘
        news_list = self.collector.collect_news(days=days)
        print(f"ğŸ“° {len(news_list)}ê°œ ë‰´ìŠ¤ ë°œê²¬")

        uploaded = 0
        skipped = 0

        for news in news_list:
            # ì¤‘ë³µ ì²´í¬
            if self.notion.check_duplicate(DATABASE_ID, news["title"]):
                print(f"â­ï¸ ì¤‘ë³µ ê±´ë„ˆë›°ê¸°: {news['title'][:30]}...")
                skipped += 1
                continue

            # ë‰´ìŠ¤ ë¶„ì„
            if use_claude and ANTHROPIC_API_KEY:
                analysis = self.analyzer.analyze_news(news["title"], news["content"])
            else:
                analysis = self.analyzer._fallback_analysis(
                    news["title"], news["content"]
                )

            # Notion ì†ì„± êµ¬ì„±
            properties = {
                "ì œëª©": {"title": [{"text": {"content": news["title"][:100]}}]},
                "ë‚ ì§œ": {"date": {"start": news["date"]}},
                "ì¶œì²˜": {"url": news["link"]},
                "ìš”ì•½": {
                    "rich_text": [
                        {"text": {"content": analysis.get("summary", "")[:200]}}
                    ]
                },
                "ê´€ë ¨ ê¸°ìˆ ": {
                    "multi_select": [
                        {"name": tech} for tech in analysis.get("technologies", [])[:5]
                    ]
                },
                "ê¸°ì—…/ê¸°ê´€": {"select": {"name": analysis.get("organization", "ê¸°íƒ€")}},
                "ì¤‘ìš”ë„": {"select": {"name": analysis.get("importance", "ğŸ“Œ ì¼ë°˜")}},
            }

            # Notionì— ì—…ë¡œë“œ
            try:
                # í˜ì´ì§€ ë‚´ìš©ì— ì‚¬ìš©í•  ë°ì´í„° (ì›ë¬¸ ë³´ì¡´)
                page_content = {
                    "summary": analysis.get("summary", ""),
                    "key_points": analysis.get("key_points", []),
                    "content": news["content"],  # ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    "link": news["link"],
                    "date": news["date"],
                    "source": news["source"],
                }

                result = self.notion.create_page(DATABASE_ID, properties, page_content)
                if "id" in result:
                    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {news['title'][:40]}...")
                    uploaded += 1
                else:
                    print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {result.get('message', 'Unknown error')}")
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")

        print(f"\nğŸ“Š ì™„ë£Œ! ì—…ë¡œë“œ: {uploaded}ê°œ, ì¤‘ë³µ ê±´ë„ˆë›°ê¸°: {skipped}ê°œ")
        return uploaded


# =============================================================================
# ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="AI ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ê¸°")
    parser.add_argument("--days", type=int, default=1, help="ìˆ˜ì§‘í•  ê¸°ê°„ (ì¼)")
    parser.add_argument(
        "--no-claude", action="store_true", help="Claude API ì‚¬ìš©í•˜ì§€ ì•ŠìŒ"
    )

    args = parser.parse_args()

    # API í‚¤ í™•ì¸
    if not NOTION_API_KEY:
        print("âŒ NOTION_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        exit(1)

    bot = AINewsBot()
    bot.run(days=args.days, use_claude=not args.no_claude)
